{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng_to_hindi_transliteration_TF.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbeF3vZe9doq",
        "colab_type": "code",
        "outputId": "ebba01fb-0b01-4814-8189-a9204068036b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26nlHSvW90rw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fef2bfb8-878d-42b2-a378-c9b8e4cf7491"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wGFtXH-LeQw",
        "colab_type": "code",
        "outputId": "31b331af-a92f-4aa0-bbf3-6d0b4b640fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# storing all the alphabets of English and the pad char to a dictionary to create OHE representation later.\n",
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "start_char = '<start>'\n",
        "end_char = '<end>'\n",
        "\n",
        "eng_alpha2index = {start_char: 0,end_char : 1}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2index)\n",
        "\n",
        "\n",
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {start_char: 0,end_char : 1}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+1\n",
        "\n",
        "print(hindi_alpha2index)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<start>': 0, '<end>': 1, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n",
            "{'<start>': 0, '<end>': 1, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iYg16lyq3Wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence_english(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = w.replace('-', ' ').replace(',', ' ')\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '@' + w + '#'\n",
        "    return w.split()\n",
        "\n",
        "def preprocess_sentence_hindi(w):\n",
        "    w = unicode_to_ascii(w.strip())\n",
        "    w = w.replace('-', ' ').replace(',', ' ')\n",
        "    cleaned_line = ''\n",
        "    for char in w:\n",
        "        if char in hindi_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "#     print(cleaned_line)\n",
        "#     cleaned_line = cleaned_line.split()\n",
        "#     print(cleaned_line)\n",
        "    cleaned_line = cleaned_line.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    cleaned_line = '@' + cleaned_line + '#'\n",
        "    return cleaned_line.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY6mGkni-LGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "def create_dataset(filename):\n",
        "\n",
        "    transliterationCorpus = ET.parse(filename).getroot()\n",
        "    lang1_words = []\n",
        "    lang2_words = []\n",
        "\n",
        "    for line in transliterationCorpus:\n",
        "                wordlist1 = preprocess_sentence_english(line[0].text) # clean English words.\n",
        "                wordlist2 = preprocess_sentence_hindi(line[1].text)# clean hindi words.\n",
        "                if len(wordlist1) != len(wordlist2):\n",
        "                    print('Skipping: ', line[0].text, ' - ', line[1].text)\n",
        "                    continue\n",
        "\n",
        "                for word in wordlist1:\n",
        "                    lang1_words.append(word)\n",
        "                for word in wordlist2:\n",
        "                    lang2_words.append(word)\n",
        "    return [lang1_words,lang2_words]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKlK5YustJ0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49bce0bf-1fab-4a82-bfa6-ea3bde2e7553"
      },
      "source": [
        "PATH = \"/content/drive/My Drive/NLP-self/Data/English_Devnagri_transliteration_data/\"\n",
        "train_data = create_dataset(PATH+'NEWS2012TrainingEnHi.xml')\n",
        "test_data = create_dataset(PATH+'NEWS2012-Testing-EnHi-1000.xml')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping:  BUSHNELL'S MUSEUM  -  बुशनेल्स म्युज़ियम\n",
            "Skipping:  I^DUKAANT  -  इंदुकांत\n",
            "Skipping:  THE AUSTRALIAN/VOGEL LITERARY AWARD  -  द ऑस्ट्रेलियन/वोगेल लिट्रेरी अवार्ड\n",
            "Skipping:  Bhaalachan_dr  -  भालचन्द्र\n",
            "Skipping:  MAN OF THE YEAR/PERSON OF THE YEAR  -  मैन ऑफ द ईयर/पर्सन ऑफ द ईयर\n",
            "Skipping:  BARHARWA JUNCTION  -  बरहरवा\n",
            "Skipping:  QUEEN ANNE'S WAR  -  क्वीन एनीज वार\n",
            "Skipping:  STATE BNK TR  -  स्टेट बैंक ऑफ त्रावणकोर\n",
            "Skipping:  IN_DRAJEET  -  इन्द्रजीत\n",
            "Skipping:  SAINT FRANCIS D'ASSISI HIGH SCHOOL  -  सेंट फ्रांसिस ड‍िअस‍ीसी हाई स्कूल\n",
            "Skipping:  SHU'A  -  शुआ\n",
            "Skipping:  SOUTH ARLINGTON CHURCH OF CHRIST  -  साउथ अर्लिंग्टन\n",
            "Skipping:  I^diyaa  -  इंडिया\n",
            "Skipping:  DAI'EI  -  दैई\n",
            "Skipping:  ISMA'IL  -  इस्मा'ईल\n",
            "Skipping:  WOMEN'S LITERATURE PRIZE  -  वुमेन्स लिट्रेचर प्राइज़\n",
            "Skipping:  KING EDWARD VII  -  किंग एडवर्ड\n",
            "Skipping:  BRAJEN_DR  -  ब्रजेन्द्र\n",
            "Skipping:  DIBANG VALLEY  -  दिबंगवैली\n",
            "Skipping:  LU'LU  -  लु'लु\n",
            "Skipping:  SAINT MICHAEL'S ACADEMY, CHENNAI  -  सेंट माइकल्स एकेडमी, चेन्नई\n",
            "Skipping:  Cadbury's  -  कैडबरीज़\n",
            "Skipping:  FRANKLIN D. ROOSEVELT  -  फ्रेंकलिन डी. रूसबेल्ट\n",
            "Skipping:  SAINT STANISLAUS HIGH SCHOOL. MUMBAI  -  सेंट स्टेनिस्लॉस हाई स्कूल, मुम्बई\n",
            "Skipping:  SAINT GEORGE'S CHANNEL  -  सेंट जॉर्ज्स चैनल\n",
            "Skipping:  I^JEENIYAR  -  इंजीनियर\n",
            "Skipping:  JITEN_DR  -  जितेन्द्र\n",
            "Skipping:  APNA KAUN ?  -  अपना कौन?\n",
            "Skipping:  ORDER OF VASA  -  ऑडर ऑफ़ द वासा\n",
            "Skipping:  P'yongyang  -  पियोंगयांग\n",
            "Skipping:  SA'IRAH  -  साइराह\n",
            "Skipping:  DOROTHY CANFIELD FISHER CHILDREN'S BOOK AWARD  -  डोरोथी कैनफील्ड फिशर चिल्ड्रन बुक अवार्ड\n",
            "Skipping:  Baidu.com  -  बेदु.कॉम\n",
            "Skipping:  AZAMNAGAR ROAD  -  आज़मनगर\n",
            "Skipping:  KING GEORGE'S WAR  -  किंग जॉर्ज्स वार\n",
            "Skipping:  ALMA ASSEMBLY OF GOD'S  -  अल्मा असेंब्ली ऑफ गॉड्स\n",
            "Skipping:  CAPE TOWN  -  केपटाउन\n",
            "Skipping:  NEW ZEALAND  -  न्यूज़ीलैंड\n",
            "Skipping:  SEA OF THE HEBRIDES  -  सी ऑफ हरब्रिड्‍स\n",
            "Skipping:  RAMCOIND  -  राम्को इंड\n",
            "Skipping:  MANEE^DR  -  मणींद्र\n",
            "Skipping:  VISHVEN_DR  -  विश्वेन्द्र\n",
            "Skipping:  KELVINGROVE ART GALLERY AND MUSEUM  -  केल्व‍िनग्रोव आर्ट एण्ड म्युज़ियम\n",
            "Skipping:  AUSTRALIAN NATIONAL UNIVERSITY  -  ऑस्ट्रेलियननेशनल यूनिवर्सिटी\n",
            "Skipping:  JAHAN AARA  -  जहाँआरा\n",
            "Skipping:  NAVABHARAT FERRO ALLOYS  -  नव भारत फ़ैरो अलॉय\n",
            "Skipping:  FA'ATETE  -  फाटेट\n",
            "Skipping:  LAHI'AH  -  लहियाह\n",
            "Skipping:  WORLD ASSOCIATION OF NEWSPAPERS'S GOLDEN PEN OF FREEDOM AWARD  -  वर्ल्ड एसोसिएशन ऑफ न्युज़पेपर्स गोल्डन पेन ऑफ फ्रीडम अवार्ड\n",
            "Skipping:  RAMA LINGESHWARA  -  रामालिंगेश्वर\n",
            "Skipping:  JACOB'S WELL  -  जैकॉब्स वेल\n",
            "Skipping:  FAKHRUN NISA  -  फखरुन्निसा\n",
            "Skipping:  SA'OOD  -  सऊद\n",
            "Skipping:  DEVEN_DR  -  देवेन्द्र\n",
            "Skipping:  IN'AM  -  इनआम\n",
            "Skipping:  SHU'AA  -  शुआ\n",
            "Skipping:  OMKARNATH THAKUR  -  ओंकार नाथ ठाकुर\n",
            "Skipping:  OPENTV  -  ओपन टीवी\n",
            "Skipping:  SHAILEN_DR  -  शैलेन्द्र\n",
            "Skipping:  SAINT XAVIERS'S SCHOOL, CHANDIGARH  -  सेंट ज़ेवियर्स स्कूल, चंडीगढ़\n",
            "Skipping:  Un_manaa  -  उन्मना\n",
            "Skipping:  ENVOY COMMUNICATIONS GROUP  -  एन्वॉय कम्युनिकेशंस\n",
            "Skipping:  CHINA'S GREAT WALL MUSEUM  -  चाइनाज ग्रेट वाल म्युज़ियम\n",
            "Skipping:  SA'IM  -  सईम\n",
            "Skipping:  WAR OF THE HOLY LEAGUE  -  वार ऑफ होली लीग\n",
            "Skipping:  ABDUL MUTA'ALEE  -  अब्दुल मुताअली\n",
            "Skipping:  CHAMPION'S CUP  -  चैंपियन्स कप\n",
            "Skipping:  PRICKETT'S FORT  -  प्रिकेट्‍स फोर्ट\n",
            "Skipping:  UNICREDITO ITALIANO.  -  यूनीक्रेडिटो इटालियानो\n",
            "Skipping:  MU'ALLA  -  मुअल्ला\n",
            "Skipping:  GOVERNOR GENERAL'S AWARD  -  गवर्नर जनरल्स अवार्ड\n",
            "Skipping:  TO'ERE  -  टोएरे\n",
            "Skipping:  SUREN_DAR  -  सुरेन्दर\n",
            "Skipping:  SAINT MARY'S CONVENT INTER COLLEGE, ALLAHABAD  -  सेंट मेरीज़ कॉंन्वेंट इंटर कॉलेज, अलाहाबाद\n",
            "Skipping:  SAINT JOHN'S HIGH SCHOOL, CHANDIGARH  -  सेंट जॉन्‍स हाई स्कूल, चंडीगढ़\n",
            "Skipping:  SIR JOHN SOANE'S MUSEUM  -  सर जॉन सॉन्स म्युज़ियम\n",
            "Skipping:  Lokamaan_y  -  लोकमान्य\n",
            "Skipping:  VAPARAISO CHURCH OF CHRIST  -  व्हापरासिओ\n",
            "Skipping:  SAUJAN_Y  -  सौजन्य\n",
            "Skipping:  PARIS CHARLES DE GAULLE  -  पेरिस रॉसे चार्ल्स डे ग्यूले\n",
            "Skipping:  PARKWAY APOSTOLIC  -  पार्क वे अपोस्टोलिक\n",
            "Skipping:  Kapee^dr  -  कपींद्र\n",
            "Skipping:  E'TEMAD  -  इत्तेमाद\n",
            "Skipping:  MU'AWWIZ  -  मुआव्विज़\n",
            "Skipping:  MAUNA LOA  -  मौनालोआ\n",
            "Skipping:  HARRY S. TRUMAN  -  हैरी एस. ट्रूमेन\n",
            "Skipping:  SAINT GEORGE'S  -  सेंट जॉर्ज\n",
            "Skipping:  MCDONALD'S  -  मैकडॉनल्ड्स\n",
            "Skipping:  MU'AZZAM  -  मुअज़्ज़म\n",
            "Skipping:  MASS MUTUAL LIFE  -  मास म्युच्युअल लाइफ़ इंश्योरेंस\n",
            "Skipping:  STATS CHIPPAC  -  स्टेट्सचिपपैक\n",
            "Skipping:  JOHN F. KENNEDY  -  जॉन एफ. केनेडी\n",
            "Skipping:  NA'IL  -  नाइल\n",
            "Skipping:  NEWFOUNDLAND  -  न्यू फाउंडलैंड\n",
            "Skipping:  LONDONHEATHROW  -  लंदन हीथ्रो\n",
            "Skipping:  KING PHILIP'S WAR  -  किंग फिलिप्स वार\n",
            "Skipping:  GAJEN_DR  -  गजेन्द्र\n",
            "Skipping:  Naren_dar  -  नरेन्दर\n",
            "Skipping:  MA'MUN  -  मैमून\n",
            "Skipping:  MU'AWIYAH  -  मुआवियाह\n",
            "Skipping:  HUKUMACHAN_D  -  हुकुमचन्द\n",
            "Skipping:  VANAKAN_YAA  -  वनकन्या\n",
            "Skipping:  RETALIX  -  रेटालिक्स लि.\n",
            "Skipping:  ATINDARJEET.  -  अतिंदरजीत\n",
            "Skipping:  SRISAILAM  -  श्री शैलम\n",
            "Skipping:  COLVIN TALUQADAR'S COLLEGE, LUCKNOW  -  कॉल्व‍िन तालुकदार्स कॉलेज, लखनऊ\n",
            "Skipping:  ZE'EV PRIZE  -  ज़ेव प्राइज़\n",
            "Skipping:  KAN_HAIYAALAAL  -  कन्हैयालाल\n",
            "Skipping:  KARA-KUM  -  काराकुम\n",
            "Skipping:  WIND RIVER  -  विंडरिवर\n",
            "Skipping:  SA'ID  -  सईद\n",
            "Skipping:  HARTFORD/BRADLEY  -  हॉर्टफोर्ड/ब्रॉड्ले\n",
            "Skipping:  NETAJI SUBHASH CHANDRA BOSE  -  नेताजी सुभाषचंद्र बोस\n",
            "Skipping:  ROCKBROOK UNITED  -  रॉकब्रुक यूनाइटेड मेथोडिस्ट\n",
            "Skipping:  WALTER SCOTT  -  वॉल्टरस्कॉट\n",
            "Skipping:  COLOURPLUS FASHIONS  -  कलर प्लस फ़ैशन्स\n",
            "Skipping:  Islip/macarthur  -  इसलिप/माकॉरथूर\n",
            "Skipping:  DUMMER'S WAR  -  ड्रमर्स वार\n",
            "Skipping:  SAINT PIERRE'S EPISCOPAL  -  सेंट पिरेस एपिस्कोपाल\n",
            "Skipping:  BAL KRISHNA  -  बालकृष्णा\n",
            "Skipping:  MU'ALLIM  -  मुअल्लिम\n",
            "Skipping:  SU'AD  -  सुआह\n",
            "Skipping:  PUGACHEV'S REBELLION  -  प्यूगाचेव्स रिबेलियन\n",
            "Skipping:  Shaile^dr  -  शैलेंद्र\n",
            "Skipping:  RAAJEN_DR  -  राजेन्द्र\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jhOoqE5viF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "4e97868f-af66-4ff2-be75-9ca8b96f7a97"
      },
      "source": [
        "train_data[0][:15],train_data[1][:15]"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['@raasavihaaree#',\n",
              "  '@deogan',\n",
              "  'road#',\n",
              "  '@shatrumardan#',\n",
              "  '@mahijuba#',\n",
              "  '@sabine#',\n",
              "  '@bill',\n",
              "  'cosby#',\n",
              "  '@rishta',\n",
              "  'kagaz',\n",
              "  'ka#',\n",
              "  '@hatim#',\n",
              "  '@shreemayi#',\n",
              "  '@farihah#',\n",
              "  '@maritime'],\n",
              " ['@रासविहारी#',\n",
              "  '@दवगन',\n",
              "  'रोड#',\n",
              "  '@शतरमरदन#',\n",
              "  '@महिजबा#',\n",
              "  '@सबिन#',\n",
              "  '@बिल',\n",
              "  'कॉसबी#',\n",
              "  '@रिशता',\n",
              "  'कागज',\n",
              "  'का#',\n",
              "  '@हातिम#',\n",
              "  '@शरीमयी#',\n",
              "  '@फरीहाह#',\n",
              "  '@मरीटाइम'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPP80gJqAKy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8b26b36-c718-4d09-83e7-1ba6d8439a2f"
      },
      "source": [
        "len(test_data[0]),len(test_data[1]),len(train_data[0])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000, 20373)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK_KmTnZ6Ne2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "983beaf4-c5ca-4982-f17f-20061871f433"
      },
      "source": [
        "train_data[0].extend(test_data[0])\n",
        "train_data[1].extend(test_data[1])\n",
        "\n",
        "print(len(train_data[0]),len(train_data[1]))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21373 21373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcu85hJ08Hc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2999e1e1-e425-42de-b54c-1eb02e5fa892"
      },
      "source": [
        "train_data[0][-1],train_data[1][-1]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('@zion#', '@जिऑन#')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9x5Ai0rM65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return word_pairs in the form of [English,Hindi]\n",
        "\n",
        "def add_start_end_char(words):\n",
        "    res = []\n",
        "    \n",
        "    start = '@'\n",
        "    end = '#'\n",
        "    for word in words:\n",
        "      new_word = start+word+end\n",
        "      res.append(new_word)\n",
        "      \n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w_pxe5Kxjzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = add_start_end_char(train_data[0]),add_start_end_char(train_data[1])\n",
        "# test_data = add_start_end_char(test_data[0]),add_start_end_char(test_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgJq_0M-yM2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d23a551c-40bb-42a1-b66f-4e71b4d69910"
      },
      "source": [
        "len(train_data[0]),len(train_data[1])"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21373, 21373)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pb7NniioydP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      for l in phrase:\n",
        "        self.vocab.update(l)\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDeBLOoMzW1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(num_examples,pairs):\n",
        "\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = WordIndex(pairs[0])\n",
        "    targ_lang = WordIndex(pairs[1])\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Spanish sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en] for en in pairs[0]]\n",
        "    \n",
        "    # English sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in hn] for hn in pairs[1]]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFHpjN3u0QGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = None\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(num_examples,train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2icIwEi40Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.idx2word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkWJ4ug94YWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "35f13535-58b5-4d97-f865-95f34427f09d"
      },
      "source": [
        "convert(inp_lang,input_tensor_train[-1])\n",
        "convert(targ_lang,target_tensor_train[-1])"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 ----> @\n",
            "5 ----> b\n",
            "4 ----> a\n",
            "7 ----> d\n",
            "4 ----> a\n",
            "10 ----> g\n",
            "4 ----> a\n",
            "21 ----> r\n",
            "4 ----> a\n",
            "1 ----> #\n",
            "2 ----> @\n",
            "36 ----> ब\n",
            "31 ----> द\n",
            "47 ----> ा\n",
            "17 ----> ग\n",
            "40 ----> र\n",
            "47 ----> ा\n",
            "1 ----> #\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRAS0_lV9Cbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c33963fa-5671-4d57-e20a-c66dc471cc5c"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.05)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20304, 20304, 1069, 1069)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxELo1OZ5J3p",
        "colab_type": "text"
      },
      "source": [
        "## Some extra : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWe5RI5MLOIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building corpus\n",
        "\n",
        "English_corpus = {}\n",
        "Hindi_corpus = {}\n",
        "\n",
        "english_alpha = list(eng_alpha2index.keys())\n",
        "hindi_alpha   = list(hindi_alpha2index.keys())\n",
        "\n",
        "for en in english_alpha:\n",
        "  English_corpus[en] = 0\n",
        "\n",
        "for hn in hindi_alpha:\n",
        "  Hindi_corpus[hn] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itzF770Xoxjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "def build_freq_dict(dictionary,datasets,n_index):\n",
        "  for data in datasets:\n",
        "    for dp in data:\n",
        "      for l in range(len(dp)):\n",
        "        dictionary[dp[l]]+= 1\n",
        "  sorted_dict = sorted(dictionary.items(), key=lambda kv: kv[1],reverse = True)\n",
        "  char_to_freq_count = collections.OrderedDict()\n",
        "  for pair in sorted_dict:\n",
        "    char_to_freq_count[pair[0]] = pair[1]\n",
        "  df_char_freq_rank = pd.DataFrame(index = char_to_freq_count.keys())\n",
        "  df_char_freq_rank['index'] = range(1,n_index+1)\n",
        "  df_char_freq_rank['count'] = char_to_freq_count.values()\n",
        "  \n",
        "  return df_char_freq_rank\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL5KJ2tvM8Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence_english(w):\n",
        "    w = unicode_to_ascii(w.strip())\n",
        "    return w\n",
        "def preprocess_sentence_hindi(w):\n",
        "    w = unicode_to_ascii(w.strip())\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHWhHCSn9vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting the letter freq and storing it in the dictionary\n",
        "x_train,y_train = train_data[0],train_data[1]\n",
        "x_test,y_test = test_data[0],test_data[1]\n",
        "\n",
        "df_char_to_freq_eng = build_freq_dict(English_corpus,[x_train,x_test],27)\n",
        "df_char_to_freq_hin = build_freq_dict(Hindi_corpus,[y_train,y_test],129)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V44qfIkYoV07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1809cbd6-5460-4070-bf5e-f406baf3966c"
      },
      "source": [
        "df_char_to_freq_eng.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>1</td>\n",
              "      <td>24237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R</th>\n",
              "      <td>2</td>\n",
              "      <td>10225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>3</td>\n",
              "      <td>9735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>4</td>\n",
              "      <td>9462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N</th>\n",
              "      <td>5</td>\n",
              "      <td>9340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  count\n",
              "A      1  24237\n",
              "R      2  10225\n",
              "I      3   9735\n",
              "E      4   9462\n",
              "N      5   9340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oepAnIRexATN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "eed408e2-0ff7-4788-8861-0d3ca8f5bd18"
      },
      "source": [
        "df_char_to_freq_hin.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ा</th>\n",
              "      <td>1</td>\n",
              "      <td>9931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>र</th>\n",
              "      <td>2</td>\n",
              "      <td>9892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>्</th>\n",
              "      <td>3</td>\n",
              "      <td>8517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>न</th>\n",
              "      <td>4</td>\n",
              "      <td>6104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ि</th>\n",
              "      <td>5</td>\n",
              "      <td>5668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  count\n",
              "ा      1   9931\n",
              "र      2   9892\n",
              "्      3   8517\n",
              "न      4   6104\n",
              "ि      5   5668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FenhaWyVxhSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eng_rep(df_char_freq_rank,eng_word):\n",
        "  rep = []\n",
        "  for l in eng_word:\n",
        "    rep.append(df_char_freq_rank['index'][l])\n",
        "  return rep\n",
        "\n",
        "def hin_rep(df_char_freq_rank,hn_word):\n",
        "  rep = []\n",
        "  for l in hn_word:\n",
        "    rep.append(df_char_freq_rank['index'][l])\n",
        "  return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7cp42NV_4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ced8989-44d3-4b9d-c1a1-0fac7c40288f"
      },
      "source": [
        "eng_rep(df_char_to_freq_eng,x_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1, 1, 7, 1, 20, 3, 6, 1, 1, 2, 4, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZnBjSYCY29l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_word_len_eng = 20\n",
        "max_word_len_hn = 21\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2zQOJqgWFtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_tensor(words,word_rep,max_len,df):\n",
        "  rep = []\n",
        "  for word in words:\n",
        "    word_tensor = word_rep(df,word)\n",
        "\n",
        "    rep.append(word_tensor)\n",
        "  pd_tensor = sequence.pad_sequences(rep,maxlen=max_len,padding='post')\n",
        "\n",
        "  return pd_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlFV4_PaLsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "049e4200-a278-4f5e-e414-e81309a9664a"
      },
      "source": [
        "word_to_tensor(['HELLO','WORLD'],eng_rep,12,df_char_to_freq_eng)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  4, 10, 10,  8,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [22,  8,  2, 10, 11,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF1-HpUrXPHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = word_to_tensor(x_train,eng_rep,max_word_len_eng,df_char_to_freq_eng)\n",
        "X_test = word_to_tensor(x_test,eng_rep,max_word_len_eng,df_char_to_freq_eng)\n",
        "Y_train = word_to_tensor(y_train,hin_rep,max_word_len_hn,df_char_to_freq_hin)\n",
        "Y_test = word_to_tensor(y_test,hin_rep,max_word_len_hn,df_char_to_freq_hin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDjuD7V5O-D",
        "colab_type": "text"
      },
      "source": [
        "## Model trannig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPfhzW_4cN2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BUFFER_SIZE = len(X_train)\n",
        "# BATCH_SIZE = 64\n",
        "# steps_per_epoch = len(X_train)//BATCH_SIZE\n",
        "# embedding_dim = 256\n",
        "# units = 1024\n",
        "# vocab_inp_size = len(list(eng_alpha2index.keys()))\n",
        "# vocab_tar_size = len(list(hindi_alpha2index.keys()))\n",
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE)\n",
        "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# BUFFER_SIZE = len(input_tensor_train)\n",
        "# BATCH_SIZE = 64\n",
        "# N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "# embedding_dim = 256\n",
        "# units = 1024\n",
        "# vocab_inp_size = len(inp_lang.word2idx)\n",
        "# vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)+1\n",
        "vocab_tar_size = len(targ_lang.word2idx)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AsixrwWEom3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5OEDecXdWq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71e03be5-5acb-47d0-cf51-754f90a88c4a"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 22]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vV7EveB-9p",
        "colab_type": "text"
      },
      "source": [
        "## New model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOufJqaGB8vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A21-fS9CDSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDC5Z0L8CEKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NDK4w4MCNHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "8990c445-f125-4113-cca8-4abb38b06f1b"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0817 14:43:50.501907 140431325484928 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0817 14:43:51.809124 140431325484928 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVKjUfuICRMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtj2E8xZCSWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jp5VyL-CSQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "b6ebbe22-5c47-4a88-9ad9-26dda331374e"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['@']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4s}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         str(batch_loss)))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss Tens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-6b710ad01699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_SoftmaxGrad\u001b[0;34m(op, grad_softmax)\u001b[0m\n\u001b[1;32m    302\u001b[0m   \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0msum_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_softmax\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_softmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum_channels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10856\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10857\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10858\u001b[0;31m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10859\u001b[0m   _execute.record_gradient(\n\u001b[1;32m  10860\u001b[0m       \"Sub\", _inputs_flat, _attrs, _result, name)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2643\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m     \u001b[0moneof_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhichOneof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0qKU-bGCSLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO6UtVctCSDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5C5zRzBCR4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvKZ_oXrB3ao",
        "colab_type": "text"
      },
      "source": [
        "## Old model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqskC77rdnCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "#     print(type(x))\n",
        "    x = self.embedding(x)\n",
        "#     print(x.shape)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crfdgaFOduHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "421ca823-e05d-49f8-9c08-d674a3dcae41"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cz_f1wVdwxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDkWwjEOd-Ao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6992affb-e47c-4aeb-d2de-44e155fa9a55"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 22, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nm06Z5ZeA_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJsui7IgeE8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8314e70-9862-4427-b26a-36963c84e2e8"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHebK0P-eHUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hhETH5heO8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok00MPnQeQ5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq5pHgRkelfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "185206e2-e855-43fe-ba3f-215c1e137b38"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0817 18:06:04.260937 140658811774848 ag_logging.py:145] Entity <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>: ValueError: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>, which Python reported as:\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "  def call(self, x, hidden):\n",
            "#     print(type(x))\n",
            "    x = self.embedding(x)\n",
            "#     print(x.shape)\n",
            "    output, state = self.gru(x, initial_state = hidden)\n",
            "    return output, state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>: ValueError: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>, which Python reported as:\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "  def call(self, x, hidden):\n",
            "#     print(type(x))\n",
            "    x = self.embedding(x)\n",
            "#     print(x.shape)\n",
            "    output, state = self.gru(x, initial_state = hidden)\n",
            "    return output, state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0817 18:06:24.550867 140658811774848 ag_logging.py:145] Entity <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>: ValueError: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>, which Python reported as:\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "  def call(self, x, hidden):\n",
            "#     print(type(x))\n",
            "    x = self.embedding(x)\n",
            "#     print(x.shape)\n",
            "    output, state = self.gru(x, initial_state = hidden)\n",
            "    return output, state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>: ValueError: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x7fecf1307080>>, which Python reported as:\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "  def call(self, x, hidden):\n",
            "#     print(type(x))\n",
            "    x = self.embedding(x)\n",
            "#     print(x.shape)\n",
            "    output, state = self.gru(x, initial_state = hidden)\n",
            "    return output, state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "Epoch 1 Batch 0 Loss 1.2760\n",
            "Epoch 1 Batch 100 Loss 0.8458\n",
            "Epoch 1 Batch 200 Loss 0.4385\n",
            "Epoch 1 Batch 300 Loss 0.2282\n",
            "Epoch 1 Loss 0.6259\n",
            "Time taken for 1 epoch 122.90636730194092 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2406\n",
            "Epoch 2 Batch 100 Loss 0.1974\n",
            "Epoch 2 Batch 200 Loss 0.1945\n",
            "Epoch 2 Batch 300 Loss 0.1429\n",
            "Epoch 2 Loss 0.2064\n",
            "Time taken for 1 epoch 79.23210859298706 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1504\n",
            "Epoch 3 Batch 100 Loss 0.1558\n",
            "Epoch 3 Batch 200 Loss 0.1419\n",
            "Epoch 3 Batch 300 Loss 0.1008\n",
            "Epoch 3 Loss 0.1463\n",
            "Time taken for 1 epoch 78.2243857383728 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1240\n",
            "Epoch 4 Batch 100 Loss 0.1254\n",
            "Epoch 4 Batch 200 Loss 0.1249\n",
            "Epoch 4 Batch 300 Loss 0.0941\n",
            "Epoch 4 Loss 0.1192\n",
            "Time taken for 1 epoch 78.4115879535675 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1092\n",
            "Epoch 5 Batch 100 Loss 0.1131\n",
            "Epoch 5 Batch 200 Loss 0.0957\n",
            "Epoch 5 Batch 300 Loss 0.0918\n",
            "Epoch 5 Loss 0.1009\n",
            "Time taken for 1 epoch 78.15291619300842 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1051\n",
            "Epoch 6 Batch 100 Loss 0.1070\n",
            "Epoch 6 Batch 200 Loss 0.0761\n",
            "Epoch 6 Batch 300 Loss 0.0680\n",
            "Epoch 6 Loss 0.0819\n",
            "Time taken for 1 epoch 78.29815220832825 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0752\n",
            "Epoch 7 Batch 100 Loss 0.0951\n",
            "Epoch 7 Batch 200 Loss 0.0673\n",
            "Epoch 7 Batch 300 Loss 0.0444\n",
            "Epoch 7 Loss 0.0724\n",
            "Time taken for 1 epoch 78.21673965454102 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0869\n",
            "Epoch 8 Batch 100 Loss 0.0832\n",
            "Epoch 8 Batch 200 Loss 0.0682\n",
            "Epoch 8 Batch 300 Loss 0.0392\n",
            "Epoch 8 Loss 0.0631\n",
            "Time taken for 1 epoch 78.37379837036133 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0596\n",
            "Epoch 9 Batch 100 Loss 0.0756\n",
            "Epoch 9 Batch 200 Loss 0.0499\n",
            "Epoch 9 Batch 300 Loss 0.0285\n",
            "Epoch 9 Loss 0.0526\n",
            "Time taken for 1 epoch 78.19786262512207 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0492\n",
            "Epoch 10 Batch 100 Loss 0.0655\n",
            "Epoch 10 Batch 200 Loss 0.0443\n",
            "Epoch 10 Batch 300 Loss 0.0321\n",
            "Epoch 10 Loss 0.0464\n",
            "Time taken for 1 epoch 78.54365396499634 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Vm9jQ7KZnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab04a341-0f5a-47b0-f985-acf42523d779"
      },
      "source": [
        "inp_lang.word2idx['r']"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnAodH2Ymg6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence_english(sentence)[0]\n",
        "\n",
        "    inputs = [inp_lang.word2idx[i] for i in sentence]\n",
        "    print(inputs)\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    print(inputs)\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['@']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '#':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bptdnTqMpbWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "#     print(predicted_sentence)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHsmy5itqt36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "#     print(type(result))\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(list(sentence))]\n",
        "    plot_attention(attention_plot, list(sentence), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ifF3zBkqvoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18980ded-c23c-4988-c93a-2ee1d522af15"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fed3d6040b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9QeTSl_qyAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "436459af-31b2-4635-b28f-98f8f39b450e"
      },
      "source": [
        "translate('zion')"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 29, 12, 18, 17, 1]\n",
            "[[ 3 29 12 18 17  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "Input: @zion#\n",
            "Predicted translation: ज ि ऑ न # न # न # न # न # न # न \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPAAAAJGCAYAAABlWQjTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3lIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB14dsZrq62o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}